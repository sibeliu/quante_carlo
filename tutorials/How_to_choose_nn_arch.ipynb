{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96776a9-0fdb-4d11-9d77-39c03c1c5644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import quante\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel, RBF, Matern\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e717911-c4ee-4c29-8187-88a52aacd03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "pd.DataFrame(iris.data).to_csv('X.csv', index=False)\n",
    "with open('y.csv', 'w') as f:\n",
    "    f.write(','.join([str(x) for x in iris.target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b38de09-d4a5-4e62-b21e-1b87909244d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    def instance(arch):\n",
    "    \n",
    "        X = pd.read_csv('X.csv')\n",
    "        with open('y.csv') as f:\n",
    "            y = f.read().split(',')\n",
    "            \n",
    "        clf = MLPClassifier(solver='sgd', max_iter=500,\n",
    "                        hidden_layer_sizes=arch[:3], alpha=arch[3], random_state=1)\n",
    "        with warnings.catch_warnings(action='ignore'):\n",
    "            cvs = cross_val_score(clf, X, y)\n",
    "        return cvs\n",
    "        \n",
    "#    print(\"Call a single instance of MLP Classifier\") \n",
    "#    print(instance((3,3)))\n",
    "    \n",
    "    hp_tune = quante.carlo(instance, [[2, 16], [2, 4], [2, 16], [.01, .99]], kernel=DotProduct()+ WhiteKernel(), n_batches=100, n_processors=4, n_iterations=5)\n",
    "    \n",
    "    p = mp.Pool()\n",
    "    start = time.time()\n",
    "    \n",
    "    session, iterations = hp_tune(p)\n",
    "    print(\"{} seconds\".format(round(time.time() - start,2)))\n",
    "    p.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe709758-2937-4306-879a-7c3cedd64b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = session.summary()\n",
    "summary['iteration_id'] = iterations\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9e244-30f9-4cef-89be-577266fda4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = summary[['score', 'iteration_id']].groupby('iteration_id').mean().plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9069a02-61b9-47f1-aec6-b4afd418e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_alpha=  [x[:3]+(round(x[3],3),) for x in summary['layers']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a462130-9b81-44c1-afef-d92f86a7ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize =(13, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.boxplot(session.score_history)\n",
    "ax.set_xticklabels(rounded_alpha)\n",
    "plt.xticks(rotation=60)\n",
    "bx = plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96158d44-10ef-4ad9-b714-82e2e81e5855",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    def instance(hp_parameter):\n",
    "    \n",
    "        X = pd.read_csv('X.csv')\n",
    "        with open('y.csv') as f:\n",
    "            y = f.read().split(',')\n",
    "            \n",
    "        clf = LogisticRegression(solver='saga', penalty = 'elasticnet', l1_ratio=hp_parameter[0], max_iter=200 )\n",
    "        with warnings.catch_warnings(action='ignore'):\n",
    "            cvs = cross_val_score(clf, X, y)\n",
    "        \n",
    "        return cvs\n",
    "        \n",
    "    \n",
    "    hp_tune = quante.carlo(instance, [[.05, .95]], kernel=DotProduct()+ WhiteKernel(), n_batches=7, n_processors=4, n_iterations=5)\n",
    "    p = mp.Pool()\n",
    "    start = time.time()\n",
    "    \n",
    "    session, iterations = hp_tune(p)\n",
    "    print(\"{} seconds\".format(round(time.time() - start,2)))\n",
    "    p.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57754c4a-c8cb-4080-8325-37980fb9bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    def instance(hp_parameter):\n",
    "    \n",
    "        X = pd.read_csv('X.csv')\n",
    "        with open('y.csv') as f:\n",
    "            y = f.read().split(',')\n",
    "            \n",
    "        clf = LogisticRegression(solver='saga', penalty = 'elasticnet', l1_ratio=hp_parameter[0], max_iter=200 )\n",
    "        with warnings.catch_warnings(action='ignore'):\n",
    "            cvs = cross_val_score(clf, X, y)\n",
    "        \n",
    "        return cvs\n",
    "        \n",
    "    \n",
    "    hp_tune = quante.carlo(instance, [[.05, .95]], kernel=DotProduct()+ WhiteKernel(), n_batches=7, n_processors=4, n_iterations=5)\n",
    "    p = mp.Pool()\n",
    "    start = time.time()\n",
    "    \n",
    "    session, iterations = hp_tune(p)\n",
    "    print(\"{} seconds\".format(round(time.time() - start,2)))\n",
    "    p.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da219579-29af-4a8e-b5fc-08a88734caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = session.summary()\n",
    "summary['iteration_id'] = iterations\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7af5140-d021-45ca-9d46-646604c49660",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = summary[['score', 'iteration_id']].groupby('iteration_id').mean().plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63a41f5-4080-4289-990b-220f682cacff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f574469-de7b-4e84-a1fa-11b201b98809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce41f27e-56e2-43cc-8670-034bebf98367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_inputs=None, arch=None, n_outputs=None):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        if n_inputs and arch and n_outputs:\n",
    "            self.make_arch(n_inputs, arch, n_outputs)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.stack(x)\n",
    "        return logits\n",
    "\n",
    "    def make_arch(self, n_inputs, arch, n_outputs, inner_function='Relu', last_function = None):\n",
    "        layers = [torch.nn.Linear(n_inputs, arch[0])]\n",
    "        for a in range(len(arch)-1):\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            layers.append(torch.nn.Linear(arch[a], arch[a+1]))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        layers.append(torch.nn.Linear(arch[-1], n_outputs))\n",
    "    #    layers.append(torch.nn.Softmax(dim=1))                    # works better without this\n",
    "\n",
    "        self.stack = torch.nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a178f-7709-4679-b837-9ebf7e4f8f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class data_loader:\n",
    "    def __init__(self,batch_pct):\n",
    "        self.X_train = pd.read_csv('X_train.csv')\n",
    "        self.X_test = pd.read_csv('X_test.csv')\n",
    "        self.y_train = pd.read_csv('y_train.csv')\n",
    "        self.y_test = pd.read_csv('y_test.csv')\n",
    "        # with open('y_train.csv') as f:\n",
    "        #     self.y_train = [float(x) for x in f.read().split(',')]\n",
    "        # with open('y_test.csv') as f:\n",
    "        #     self.y_test = [float(x) for x in f.read().split(',')]\n",
    "\n",
    "        self.train_batch_size = int(np.floor(self.X_train.shape[0] * batch_pct))\n",
    "        self.test_batch_size = int(np.floor(self.X_test.shape[0] * batch_pct))\n",
    "        self.batch_pct = batch_pct\n",
    "        \n",
    "    def get_batch(self,batch):\n",
    "        if batch < 0:\n",
    "            return self.X_test, self.y_test\n",
    "        else:\n",
    "            return self.X_train[(batch*self.train_batch_size):((batch+1)*self.train_batch_size)],\\\n",
    "                   self.y_train[(batch*self.train_batch_size):((batch+1)*self.train_batch_size)]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b177b3d5-ddfb-435a-9dc7-0063178f8547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mp.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443de3fb-c36e-42db-aee4-26bc77fa37e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    train_batch_size = 100\n",
    "    n_batches = 4\n",
    "    n_iterations = 5\n",
    "    input_layer_size = 28*28\n",
    "    output_layer_size = 10\n",
    "\n",
    "    def instance(p):\n",
    "\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        device = 'cuda:'+str(p['thread_id'])\n",
    "        model = NeuralNetwork(input_layer_size, p['next_points'], n_outputs=output_layer_size)\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "        loss_history = []\n",
    "        model.train()\n",
    "\n",
    "        loader = data_loader(.05)\n",
    "    \n",
    "        for i in range(n_iterations):\n",
    "            for batch in range(n_batches):\n",
    "                X_train, y_train = loader.get_batch(batch)\n",
    "\n",
    "                X_train_torch = torch.tensor(X_train.values, dtype=torch.float)\n",
    "                y_train_torch = torch.tensor(y_train.values, dtype=torch.float)\n",
    "\n",
    "                X = X_train_torch.to(device)\n",
    "                y = y_train_torch.to(device)\n",
    "\n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "        \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                loss_history.append(loss.item())\n",
    "\n",
    "        X_test, y_test = loader.get_batch(-1)\n",
    "\n",
    "        model.eval()\n",
    "        X_test_torch = torch.tensor(X_test.values, dtype=torch.float)\n",
    "        y_test_torch = torch.tensor(y_test.values, dtype=torch.float)\n",
    "\n",
    "        X = X_test_torch.to(device)\n",
    "        y = y_test_torch.to(device)\n",
    "        pred = model(X)\n",
    "\n",
    "\n",
    "        return loss.item()#loss.item()#.detatch()\n",
    "    \n",
    "    def wrapper(x):\n",
    "        return instance(x)\n",
    "    #k = instance({'next_points': (17, 13), 'thread_id': 1})\n",
    "    hp_tune = quante.carlo(worker.instance, [[64, 128], [64, 128], [64, 128]], kernel=DotProduct()+ WhiteKernel(), \n",
    "                            n_batches=7, n_processors=4, n_iterations=5, keep_thread_id=True)\n",
    "    p = mp.Pool(processes = 4)\n",
    "    \n",
    "#     start = time.time()\n",
    "    \n",
    "# #    session, iterations = hp_tune(p)\n",
    "    session = hp_tune(p)\n",
    "#     print(\"{} seconds\".format(round(time.time() - start,2)))\n",
    "#     p.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227b4c91-7dbf-4b17-ab68-c1e3def7f173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.layer_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8f7fb4-671e-4b0b-bbdd-bb8ed7b1124a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = data_loader(.05)\n",
    "x,y=df.get_batch(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc2bbf1-0dba-426e-b4bd-11cbda8c3c0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    train_batch_size = 1000\n",
    "    n_batches = 4\n",
    "    n_iterations = 10\n",
    "    input_layer_size = 28*28\n",
    "    output_layer_size = 10\n",
    "    \n",
    "    def instance(arch):\n",
    "\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        device = 'cuda:'+str(arch)\n",
    "        model = NeuralNetwork(input_layer_size, (64, 64), output_layer_size)\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "        loss_history = []\n",
    "        model.train()\n",
    "\n",
    "        loader = data_loader(.05)\n",
    "        batch = 1\n",
    "        X_train, y_train = loader.get_batch(batch)\n",
    "\n",
    "        #X_train_torch = torch.tensor(X_train.values, dtype=torch.float)\n",
    "        #y_train_torch = torch.tensor(y_train, dtype=torch.float)\n",
    "#         for i in range(n_iterations):\n",
    "#             for batch in range(n_batches):\n",
    "#                 X_train, y_train = loader.get_batch(batch)\n",
    "\n",
    "#                 X_train_torch = torch.tensor(X_train.values, dtype=torch.float)\n",
    "#                 y_train_torch = torch.tensor(y_train.values, dtype=torch.float)\n",
    "\n",
    "#                 X = X_train_torch.to(device)\n",
    "#                 y = y_train_torch.to(device)\n",
    "\n",
    "#                 pred = model(X)\n",
    "#                 loss = loss_fn(pred, y)\n",
    "        \n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss_history.append(loss.item())\n",
    "\n",
    "#         X_test, y_test = loader.get_batch(-1)\n",
    "\n",
    "#         model.eval()\n",
    "#         X_test_torch = torch.tensor(X_test.values, dtype=torch.float)\n",
    "#         y_test_torch = torch.tensor(y_test.values, dtype=torch.float)\n",
    "\n",
    "#         X = X_test_torch.to(device)\n",
    "#         y = y_test_torch.to(device)\n",
    "#         pred = model(X)\n",
    "\n",
    "        return X_train.values, y_train.values\n",
    "        return 0#loss.item()#.detatch()\n",
    "    \n",
    "    \n",
    "    \n",
    "    hp_tune = quante.carlo(instance, [[64, 128], [64, 128], [64, 128]], kernel=DotProduct()+ WhiteKernel(), \n",
    "                           n_batches=7, n_processors=4, n_iterations=5, keep_thread_id=True)\n",
    "    p = mp.Pool()\n",
    "    start = time.time()\n",
    "    \n",
    "#    session, iterations = hp_tune(p)\n",
    "    session = hp_tune(p)\n",
    "\n",
    "    print(\"{} seconds\".format(round(time.time() - start,2)))\n",
    "    results = p.map(instance, range(4))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d50151-40cc-4d54-9483-885b43e37f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de65d28e-4996-4274-b9b3-6b80575c0870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_torch = torch.tensor(results[0][0], dtype=torch.float)\n",
    "X_train_torch = torch.tensor(results[0][1], dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc3608c-9b3c-48aa-97f7-b23905720377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "map_this = [tuple([i] +list(session.next_points[i])) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1a7c33-60a0-4338-91b0-78d2c0a32ede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.tensor(range(4), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dec376-d2c8-4141-9dff-bc92f88a0cba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = NeuralNetwork(input_layer_size, , n_outputs=output_layer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d3e554-12da-4340-8d63-cc8bf33d2f85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a3704a-a948-49f4-9474-5a3a3747eac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
